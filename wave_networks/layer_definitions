import torch as t
import torch.nn as nn
import torch.nn.functional as F
import math
from torch import Tensor
from jaxtyping import Float

##############################################################
# 1. Spectro-Temporal Layers                                 #
# Input: Audio timeseries: [B, T]                            #
# Output: Time, frequency plane with C kernels: [B, C, T, S] #
#                                                            #
# Fixed or learned kernel that gets broadcasted to           #
# scales 2^j for 1, 2, ..., 2^j < T / kernel_size            #
##############################################################

class WaveletLayer(nn.Module):
    def __init__(self, S: int | None, kernel_size: int):
        super().__init__()
        self.S = S
        self.kernel_size = kernel_size
        time = t.linspace(-3, 3, kernel_size)
        self.kernel = t.cos(5.0 * time) * t.exp(-time ** 2 / 2)

    def forward(self, x: Float[Tensor, "B T"]) -> Float[Tensor, "B 1 T S"]:
        T = x.shape[1]
        if self.S is not None:
            n_scales = self.S
        else:
            n_scales = max(1, int(t.log2(T / self.kernel_size)))
        x = x.unsqueeze(1)  # [B, 1, T]
        
        outputs = []
        for j in range(n_scales):
            scale = 2 ** j
            padding = (self.kernel_size - 1) * scale // 2
            out = F.conv1d(x, self.kernel, dilation=scale, padding=padding)
            out = out / scale
            outputs.append(out.unsqueeze(-1))  # [B, 1, T, 1]
        return t.cat(outputs, dim=-1)  # [B, 1, T, S]

    
class LearnableWaveletLayer(nn.Module):
    def __init__(self, S: int | None, kernel_size: int, output_channels: int):
        super().__init__()
        self.S = S
        self.kernel_size = kernel_size
        self.output_channels = output_channels
        self.kernel = nn.Parameter(
            t.randn(output_channels, 1, kernel_size)
        )

    def forward(self, x: Float[Tensor, "B T"]) -> Float[Tensor, "B C T S"]:
        T = x.shape[1]
        if self.S is not None:
            n_scales = self.S
        else:
            n_scales = max(1, int(t.log2(T / self.kernel_size)))
        x = x.unsqueeze(1)  # [B, 1, T]

        outputs = []
        for j in range(n_scales):
            scale = 2 ** j
            padding = (self.kernel_size - 1) * scale // 2
            out = F.conv1d(x, self.kernel, dilation=scale, padding=padding)
            out = out / scale
            outputs.append(out.unsqueeze(-1))  # [B, C, T, 1]
        return t.cat(outputs, dim=-1)  # [B, C, T, S]


##############################################################
# 2. Convolutional Layers                                    #
# Input: Time, frequency plane with C kernels: [B, C, T, S]  #
# Output: Time, frequency plane with C kernels: [B, C, T, S] #
#                                                            #
#           
# scales 2^j for 1, 2, ..., 2^j < T / kernel_size            #
##############################################################


class ScaleEquivariantTimeConvolutionLayer(nn.Module):
    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int):
        super().__init__()
        # Conv2d with kernel_size=(1, kernel_size) to convolve over T only
        self.conv = nn.Conv2d(
            in_channels,
            out_channels,
            kernel_size=(1, kernel_size),
            stride=(1, stride)
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Apply convolution over time for each scale separately
        return self.conv(x)

class TimeConvolutionLayer(nn.Module):
    """
    Standard 2D convolutional layer over time and scale dimensions.
    Input: [batch, in_channels, S, T]
    Output: [batch, out_channels, S2, T2]
    """
    def __init__(self, in_channels: int, out_channels: int, kernel_size_T: int, kernel_size_S: int, stride: int):
        super().__init__()
        self.conv = nn.Conv2d(
            in_channels,
            out_channels,
            kernel_size=(kernel_size_S, kernel_size_T),
            stride=(1, stride)
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.conv(x)

# 3. Audio Classifier

class AudioClassifier(nn.Module):
    """
    Modular neural network for audio classification.
    Input: [batch, 1, T]
    Output: [batch, n_classes]
    """
    def __init__(self, first_layer, conv_layers: list, n_classes: int):
        super().__init__()
        self.first_layer = first_layer
        self.conv_layers = nn.ModuleList(conv_layers)
        # Batch normalization for first layer
        self.bn_first = nn.BatchNorm2d(
            first_layer.output_channels if hasattr(first_layer, 'output_channels') else 1
        )
        # Batch normalization for convolutional layers
        self.bn_layers = nn.ModuleList([
            nn.BatchNorm2d(layer.out_channels) for layer in conv_layers
        ])
        self.n_classes = n_classes
        # Ensure the last conv layer has out_channels=n_classes
        assert conv_layers[-1].out_channels == n_classes, "Last conv layer must have out_channels=n_classes"

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # First layer: Spectro-Temporal
        x = self.first_layer(x)
        x = self.bn_first(x)
        x = F.relu(x)
        # Convolutional layers
        for conv, bn in zip(self.conv_layers, self.bn_layers):
            x = conv(x)
            x = bn(x)
            x = F.relu(x)
        # Mean pooling over S and T dimensions
        x = torch.mean(x, dim=[2, 3])  # [batch, n_classes]
        # Softmax
        x = F.softmax(x, dim=1)
        return x

# Example Usage
if __name__ == "__main__":
    # Parameters
    S = 5
    kernel_size = 2
    output_channels_first = 16
    n_classes = 10
    T = 1024  # Example input length

    # Define layers
    first_layer = LearnableWaveletLayer(S=S, kernel_size=kernel_size, output_channels=output_channels_first)
    conv_layers = [
        ScaleEquivariantTimeConvolutionLayer(
            in_channels=output_channels_first,
            out_channels=32,
            kernel_size=3,
            stride=1
        ),
        TimeConvolutionLayer(
            in_channels=32,
            out_channels=n_classes,
            kernel_size_T=3,
            kernel_size_S=3,
            stride=1
        )
    ]

    # Initialize model
    model = AudioClassifier(first_layer, conv_layers, n_classes)

    # Test with dummy input
    x = torch.randn(2, 1, T)  # [batch=2, channels=1, T]
    output = model(x)
    print(f"Output shape: {output.shape}")  # Expected: [2, n_classes]